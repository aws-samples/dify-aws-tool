{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker VLLM endpoint example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Define some variables\n",
    "\n",
    "The byoc will build and store a vllm endpoint docker image in you ECR private repo (for example `sagemaker_endpoint/vllm`), you need to define the following variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "INSTANCE_TYPE = \"ml.g5.2xlarge\"\n",
    "VLLM_VERSION = \"v0.6.4.post1\"\n",
    "REPO_NAMESPACE = \"sagemaker_endpoint/vllm\"\n",
    "ACCOUNT = !aws sts get-caller-identity --query Account --output text\n",
    "REGION = !aws configure get region\n",
    "ACCOUNT = ACCOUNT[0]\n",
    "REGION = REGION[0]\n",
    "if REGION.startswith(\"cn\"):\n",
    "    # this is a example repo port from vllm/vllm-openai, you can create your own docker image in your global region account\n",
    "    VLLM_REPO = \"public.ecr.aws/y0a9p9k0/vllm/vllm-openai\"\n",
    "    CONTAINER = f\"{ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com.cn/{REPO_NAMESPACE}:{VLLM_VERSION}\"\n",
    "else:\n",
    "    VLLM_REPO = \"vllm/vllm-openai\"\n",
    "    CONTAINER = f\"{ACCOUNT}.dkr.ecr.{REGION}.amazonaws.com/{REPO_NAMESPACE}:{VLLM_VERSION}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the container\n",
    "\n",
    "Endpoint starting codes are in `app/`. The script will build and push to ecr. \n",
    "\n",
    "**The docker only need to be built once**, and after that, when deploying other endpoints, the same docker image can be shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = f\"VLLM_REPO={VLLM_REPO} VLLM_VERSION={VLLM_VERSION} REPO_NAMESPACE={REPO_NAMESPACE} ACCOUNT={ACCOUNT} REGION={REGION} bash ./build_and_push.sh\"\n",
    "print(\"Runging:\", cmd)\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy on SageMaker\n",
    "\n",
    "define the model and deploy on SageMaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U boto3 sagemaker transformers huggingface_hub modelscopex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Init SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sess.default_bucket()\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Download and upload model file\n",
    "\n",
    "Firstly, you need to prepare model weights and upload to S3. You can download from HuggingFace, ModelScope or upload your own model. \n",
    "\n",
    "If you want vllm to automatically pull the model when it starts, this step can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = MODEL_ID.replace(\"/\", \"-\").replace(\".\", \"-\")\n",
    "local_model_path = os.environ['HOME'] + \"/models/\" + model_name\n",
    "s3_model_path = f\"s3://{default_bucket}/models/\" + model_name\n",
    "\n",
    "%mkdir -p code {local_model_path}\n",
    "\n",
    "print(\"local_model_path:\", local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Option 1: Global region (download from HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download --resume-download {MODEL_ID} --local-dir {local_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Option 2: China region  (download from ModelScope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!modelscope download --local_dir {local_model_path} {MODEL_ID} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync {local_model_path} {s3_model_path}\n",
    "print(\"s3_model_path:\", s3_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Prepare vllm start scripts\n",
    "\n",
    "Then you need to a write the vllm starting scripts for endpoint, the container will automatically use the `start.sh` as the entrypont.\n",
    "\n",
    "Please carefully modify the startup script file as needed, such as the model running parameter information. All parameters can be referenced at [https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)\n",
    "\n",
    "Here is a simple script that pulling a model from S3 and starting a vllm server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_model_name = sagemaker.utils.name_from_base(model_name, short=True)\n",
    "local_code_path = endpoint_model_name\n",
    "s3_code_path = f\"s3://{default_bucket}/endpoint_code/vllm_byoc/{endpoint_model_name}.tar.gz\"\n",
    "\n",
    "%mkdir -p {local_code_path}\n",
    "\n",
    "print(\"local_code_path:\", local_code_path)\n",
    "\n",
    "with open(f\"{local_code_path}/start.sh\", \"w\") as f:\n",
    "    f.write(f\"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# download model to local\n",
    "s5cmd sync {s3_model_path}/* /opt/ml/modelfile/\n",
    "\n",
    "\n",
    "# the start script need to be adjust as you needed\n",
    "# port needs to be $SAGEMAKER_BIND_TO_PORT\n",
    "\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\\\n",
    "    --port $SAGEMAKER_BIND_TO_PORT \\\\\n",
    "    --trust-remote-code \\\\\n",
    "    --model /opt/ml/modelfile/ \\\\\n",
    "    --max-model-len 19280\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -f {local_code_path}.tar.gz\n",
    "!tar czvf {local_code_path}.tar.gz {local_code_path}/\n",
    "!aws s3 cp {local_code_path}.tar.gz {s3_code_path}\n",
    "print(\"s3_code_path:\", s3_code_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Deploy endpoint on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 0. create model\n",
    "\n",
    "# endpoint_model_name already defined in above step\n",
    "\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName=endpoint_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": CONTAINER,\n",
    "        \"ModelDataUrl\": s3_code_path\n",
    "    },\n",
    "    \n",
    ")\n",
    "print(create_model_response)\n",
    "print(\"endpoint_model_name:\", endpoint_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1. create endpoint config\n",
    "\n",
    "endpoint_config_name = sagemaker.utils.name_from_base(model_name, short=True)\n",
    "\n",
    "endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": endpoint_model_name,\n",
    "            \"InstanceType\": INSTANCE_TYPE,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 1000,\n",
    "            # \"EnableSSMAccess\": True,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(endpoint_config_response)\n",
    "print(\"endpoint_config_name:\", endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2. create endpoint\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(model_name, short=True)\n",
    "\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(create_endpoint_response)\n",
    "print(\"endpoint_config_name:\", endpoint_name)\n",
    "while 1:\n",
    "    status = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)[\"EndpointStatus\"]\n",
    "    if status != \"Creating\":\n",
    "        break\n",
    "    print(datetime.now().strftime('%Y%m%d-%H:%M:%S') + \" status: \" + status)\n",
    "    time.sleep(60)\n",
    "print(\"Endpoint created:\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test\n",
    "\n",
    "You can invoke your model with SageMaker runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a quick sort in python\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Message api non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "payload = {\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Message api stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            data = json.loads(match.group(1).strip())\n",
    "            last_idx = match.span()[1]\n",
    "            print(data[\"choices\"][0][\"delta\"][\"content\"], end=\"\")\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Completion api non-stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "payload = {\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response['Body'].read())[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Completion api stream mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "buffer = \"\"\n",
    "for t in response['Body']:\n",
    "    buffer += t[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "    last_idx = 0\n",
    "    for match in re.finditer(r'^data:\\s*(.+?)(\\n\\n)', buffer):\n",
    "        try:\n",
    "            data = json.loads(match.group(1).strip())\n",
    "            last_idx = match.end()\n",
    "            # print(data)\n",
    "            print(data[\"choices\"][0][\"text\"], end=\"\")\n",
    "        except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "            pass\n",
    "    buffer = buffer[last_idx:]\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
